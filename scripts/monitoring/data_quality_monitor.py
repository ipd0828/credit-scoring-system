"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ–µ–∫—Ç–µ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç:
1. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–±—Ä–æ—Å–æ–≤
3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging
import json
import warnings

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
from scipy import stats
from scipy.stats import zscore
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings("ignore")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/data_quality_monitoring.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DataQualityMonitor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        """
        self.config = config or self._default_config()
        logger.info("–ú–æ–Ω–∏—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _default_config(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        return {
            "missing_threshold": 0.1,  # –ü–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (10%)
            "outlier_threshold": 3.0,  # –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤ (z-score)
            "correlation_threshold": 0.95,  # –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            "duplicate_threshold": 0.05,  # –ü–æ—Ä–æ–≥ –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (5%)
            "min_unique_ratio": 0.1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            "max_skewness": 2.0,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è
            "max_kurtosis": 3.0,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç–∫—Å—Ü–µ—Å—Å
        }
    
    def check_missing_values(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        
        missing_info = {
            "total_missing": data.isnull().sum().sum(),
            "missing_percentage": (data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100,
            "columns_with_missing": {},
            "critical_columns": [],
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            missing_counts = data.isnull().sum()
            missing_percentages = (missing_counts / len(data)) * 100
            
            for col in data.columns:
                missing_count = missing_counts[col]
                missing_pct = missing_percentages[col]
                
                missing_info["columns_with_missing"][col] = {
                    "count": int(missing_count),
                    "percentage": round(missing_pct, 2)
                }
                
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
                if missing_pct > self.config["missing_threshold"] * 100:
                    missing_info["critical_columns"].append({
                        "column": col,
                        "missing_percentage": round(missing_pct, 2)
                    })
            
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {missing_info['total_missing']} "
                       f"({missing_info['missing_percentage']:.2f}%)")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {e}")
            missing_info["error"] = str(e)
        
        return missing_info
    
    def check_outliers(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—ã–±—Ä–æ—Å—ã –≤ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤...")
        
        outlier_info = {
            "columns_with_outliers": {},
            "total_outliers": 0,
            "critical_columns": [],
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            
            for col in numeric_columns:
                if data[col].notna().sum() > 0:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –Ω–µ–ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    # Z-score –º–µ—Ç–æ–¥
                    z_scores = np.abs(zscore(data[col].dropna()))
                    outliers = z_scores > self.config["outlier_threshold"]
                    
                    outlier_count = outliers.sum()
                    outlier_percentage = (outlier_count / len(data[col].dropna())) * 100
                    
                    outlier_info["columns_with_outliers"][col] = {
                        "count": int(outlier_count),
                        "percentage": round(outlier_percentage, 2),
                        "mean_z_score": round(z_scores.mean(), 2),
                        "max_z_score": round(z_scores.max(), 2)
                    }
                    
                    outlier_info["total_outliers"] += outlier_count
                    
                    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–º –≤—ã–±—Ä–æ—Å–æ–≤
                    if outlier_percentage > 5:  # –ë–æ–ª–µ–µ 5% –≤—ã–±—Ä–æ—Å–æ–≤
                        outlier_info["critical_columns"].append({
                            "column": col,
                            "outlier_percentage": round(outlier_percentage, 2)
                        })
            
            logger.info(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–æ—Å–æ–≤: {outlier_info['total_outliers']}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤—ã–±—Ä–æ—Å–æ–≤: {e}")
            outlier_info["error"] = str(e)
        
        return outlier_info
    
    def check_data_distribution(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö...")
        
        distribution_info = {
            "columns_distribution": {},
            "skewed_columns": [],
            "high_kurtosis_columns": [],
            "low_variance_columns": [],
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            
            for col in numeric_columns:
                if data[col].notna().sum() > 10:  # –ú–∏–Ω–∏–º—É–º 10 –∑–Ω–∞—á–µ–Ω–∏–π
                    values = data[col].dropna()
                    
                    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    stats_info = {
                        "mean": round(values.mean(), 4),
                        "std": round(values.std(), 4),
                        "min": round(values.min(), 4),
                        "max": round(values.max(), 4),
                        "skewness": round(stats.skew(values), 4),
                        "kurtosis": round(stats.kurtosis(values), 4),
                        "variance": round(values.var(), 4)
                    }
                    
                    distribution_info["columns_distribution"][col] = stats_info
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Å–∏–º–º–µ—Ç—Ä–∏—é
                    if abs(stats_info["skewness"]) > self.config["max_skewness"]:
                        distribution_info["skewed_columns"].append({
                            "column": col,
                            "skewness": stats_info["skewness"]
                        })
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Å—Ü–µ—Å—Å
                    if abs(stats_info["kurtosis"]) > self.config["max_kurtosis"]:
                        distribution_info["high_kurtosis_columns"].append({
                            "column": col,
                            "kurtosis": stats_info["kurtosis"]
                        })
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∏–∑–∫—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é
                    if stats_info["variance"] < 0.01:  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
                        distribution_info["low_variance_columns"].append({
                            "column": col,
                            "variance": stats_info["variance"]
                        })
            
            logger.info(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(numeric_columns)} —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π: {e}")
            distribution_info["error"] = str(e)
        
        return distribution_info
    
    def check_data_integrity(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö...")
        
        integrity_info = {
            "duplicate_rows": data.duplicated().sum(),
            "duplicate_percentage": (data.duplicated().sum() / len(data)) * 100,
            "duplicate_columns": [],
            "constant_columns": [],
            "highly_correlated_columns": [],
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫
            logger.info(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫: {integrity_info['duplicate_rows']} "
                       f"({integrity_info['duplicate_percentage']:.2f}%)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–ª–æ–Ω–æ–∫
            for i, col1 in enumerate(data.columns):
                for j, col2 in enumerate(data.columns[i+1:], i+1):
                    if data[col1].equals(data[col2]):
                        integrity_info["duplicate_columns"].append({
                            "column1": col1,
                            "column2": col2
                        })
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            for col in data.columns:
                if data[col].nunique() <= 1:
                    integrity_info["constant_columns"].append(col)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é
            numeric_data = data.select_dtypes(include=[np.number])
            if len(numeric_data.columns) > 1:
                corr_matrix = numeric_data.corr().abs()
                
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        col1 = corr_matrix.columns[i]
                        col2 = corr_matrix.columns[j]
                        corr_value = corr_matrix.iloc[i, j]
                        
                        if corr_value > self.config["correlation_threshold"]:
                            integrity_info["highly_correlated_columns"].append({
                                "column1": col1,
                                "column2": col2,
                                "correlation": round(corr_value, 4)
                            })
            
            logger.info(f"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(integrity_info['constant_columns'])}")
            logger.info(f"–í—ã—Å–æ–∫–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä: {len(integrity_info['highly_correlated_columns'])}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {e}")
            integrity_info["error"] = str(e)
        
        return integrity_info
    
    def check_categorical_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
        
        Args:
            data: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        categorical_info = {
            "categorical_columns": {},
            "high_cardinality_columns": [],
            "low_cardinality_columns": [],
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            categorical_columns = data.select_dtypes(include=['object', 'category']).columns
            
            for col in categorical_columns:
                unique_count = data[col].nunique()
                unique_ratio = unique_count / len(data)
                
                categorical_info["categorical_columns"][col] = {
                    "unique_count": unique_count,
                    "unique_ratio": round(unique_ratio, 4),
                    "most_common": data[col].mode().iloc[0] if not data[col].mode().empty else None,
                    "most_common_count": data[col].value_counts().iloc[0] if not data[col].empty else 0
                }
                
                # –í—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
                if unique_ratio > 0.5:
                    categorical_info["high_cardinality_columns"].append({
                        "column": col,
                        "unique_ratio": round(unique_ratio, 4)
                    })
                
                # –ù–∏–∑–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
                if unique_ratio < self.config["min_unique_ratio"]:
                    categorical_info["low_cardinality_columns"].append({
                        "column": col,
                        "unique_ratio": round(unique_ratio, 4)
                    })
            
            logger.info(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(categorical_columns)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            categorical_info["error"] = str(e)
        
        return categorical_info
    
    def generate_quality_report(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "data_shape": data.shape,
            "data_types": data.dtypes.to_dict(),
            "overall_quality_score": 0.0,
            "issues": [],
            "recommendations": []
        }
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            missing_check = self.check_missing_values(data)
            outlier_check = self.check_outliers(data)
            distribution_check = self.check_data_distribution(data)
            integrity_check = self.check_data_integrity(data)
            categorical_check = self.check_categorical_data(data)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ—Ç—á–µ—Ç
            report["missing_values"] = missing_check
            report["outliers"] = outlier_check
            report["distributions"] = distribution_check
            report["data_integrity"] = integrity_check
            report["categorical_data"] = categorical_check
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
            quality_score = 100.0
            
            # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã
            if missing_check.get("critical_columns"):
                quality_score -= len(missing_check["critical_columns"]) * 5
                report["issues"].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {len(missing_check['critical_columns'])}")
            
            if outlier_check.get("critical_columns"):
                quality_score -= len(outlier_check["critical_columns"]) * 3
                report["issues"].append(f"–ö–æ–ª–æ–Ω–∫–∏ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏: {len(outlier_check['critical_columns'])}")
            
            if integrity_check.get("duplicate_percentage", 0) > 5:
                quality_score -= 10
                report["issues"].append(f"–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {integrity_check['duplicate_percentage']:.2f}%")
            
            if integrity_check.get("constant_columns"):
                quality_score -= len(integrity_check["constant_columns"]) * 2
                report["issues"].append(f"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {len(integrity_check['constant_columns'])}")
            
            report["overall_quality_score"] = max(0, quality_score)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if missing_check.get("critical_columns"):
                report["recommendations"].append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ –∏–ª–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å –≤—ã—Å–æ–∫–∏–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤")
            
            if outlier_check.get("critical_columns"):
                report["recommendations"].append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –≤—ã–±—Ä–æ—Å—ã –≤ –¥–∞–Ω–Ω—ã—Ö")
            
            if integrity_check.get("duplicate_rows", 0) > 0:
                report["recommendations"].append("–£–¥–∞–ª–∏—Ç–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫")
            
            if integrity_check.get("constant_columns"):
                report["recommendations"].append("–£–¥–∞–ª–∏—Ç–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏")
            
            if integrity_check.get("highly_correlated_columns"):
                report["recommendations"].append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")
            
            logger.info(f"–û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω. –°–∫–æ—Ä: {report['overall_quality_score']:.1f}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
            report["error"] = str(e)
            report["overall_quality_score"] = 0
        
        return report
    
    def save_quality_report(self, report: Dict[str, Any], output_path: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            report: –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        try:
            output_file = Path(output_path) / f"data_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"–û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö."""
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    data_path = "data/processed/X_train.csv"
    output_path = "monitoring/reports"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    if not Path(data_path).exists():
        logger.error(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        return 1
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}")
        data = pd.read_csv(data_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {data.shape[0]} —Å—Ç—Ä–æ–∫ –∏ {data.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä
        monitor = DataQualityMonitor()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = monitor.generate_quality_report(data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        monitor.save_quality_report(report, output_path)
        
        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
        print(f"\nüìä –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –î–ê–ù–ù–´–•")
        print(f"–í—Ä–µ–º—è: {report['timestamp']}")
        print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {report['data_shape']}")
        print(f"–°–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞: {report['overall_quality_score']:.1f}/100")
        print(f"–ü—Ä–æ–±–ª–µ–º: {len(report.get('issues', []))}")
        
        if report.get("issues"):
            print("\n‚ö†Ô∏è  –ü–†–û–ë–õ–ï–ú–´:")
            for issue in report["issues"]:
                print(f"  ‚Ä¢ {issue}")
        
        if report.get("recommendations"):
            print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            for rec in report["recommendations"]:
                print(f"  ‚Ä¢ {rec}")
        
        return 0 if report["overall_quality_score"] > 70 else 1
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ main: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
